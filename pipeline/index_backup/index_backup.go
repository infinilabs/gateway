// Copyright (C) INFINI Labs & INFINI LIMITED.
//
// The INFINI Framework is offered under the GNU Affero General Public License v3.0
// and as commercial software.
//
// For commercial licensing, contact us at:
//   - Website: infinilabs.com
//   - Email: hello@infini.ltd
//
// Open Source licensed under AGPL V3:
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program. If not, see <http://www.gnu.org/licenses/>.

/* Copyright Â© INFINI Ltd. All rights reserved.
 * web: https://infinilabs.com
 * mail: hello#infini.ltd */

package index_backup

import (
	"fmt"
	"os"
	path2 "path"
	"path/filepath"
	"runtime"
	"time"

	log "github.com/cihub/seelog"
	"github.com/emirpasic/gods/sets/hashset"
	"github.com/fsnotify/fsnotify"
	"infini.sh/framework/core/config"
	"infini.sh/framework/core/elastic"
	"infini.sh/framework/core/errors"
	"infini.sh/framework/core/global"
	"infini.sh/framework/core/kv"
	"infini.sh/framework/core/pipeline"
	"infini.sh/framework/core/s3"
	"infini.sh/framework/core/util"
)

type Config struct {
	Elasticsearch          string `config:"elasticsearch"`
	Index                  string `config:"index"`
	UploadToS3             bool   `config:"upload_to_s3"`
	UploadToFSRepo         bool   `config:"upload_to_fs_repo"`
	LocalRepoPath          string `config:"local_repo_path"`
	UploadIntervalInSecond int    `config:"upload_interval_in_second"`

	S3 struct {
		Async    bool   `config:"async"`
		Server   string `config:"server"`
		Location string `config:"location"`
		Bucket   string `config:"bucket"`
	} `config:"s3"`
}

type IndexBackupProcessor struct {
	config      *Config
	skipFiles   *hashset.Set
	watch       *fsnotify.Watcher
	changes     map[string]*ChangedItem
	changesChan chan *ChangedItem
}

var signalChannel = make(chan bool, 1)

func init() {
	pipeline.RegisterProcessorPlugin("index_backup", New)
}

type ChangedItem struct {
	IndexUUID string
	SegmentID string
	FilePath  string
	FileName  string
	Timestamp time.Time
}

func New(c *config.Config) (pipeline.Processor, error) {
	cfg := Config{
		UploadIntervalInSecond: 5,
	}

	if err := c.Unpack(&cfg); err != nil {
		log.Error(err)
		return nil, fmt.Errorf("failed to unpack the configuration of flow_runner processor: %s", err)
	}
	runner := IndexBackupProcessor{config: &cfg}
	runner.skipFiles = hashset.New()
	runner.skipFiles.Add("write.lock")
	runner.skipFiles.Add(".DS_Store")

	runner.changes = map[string]*ChangedItem{}
	runner.changesChan = make(chan *ChangedItem, 100)

	var err error
	runner.watch, err = fsnotify.NewWatcher()
	if err != nil {
		panic(err)
	}

	global.RegisterShutdownCallback(func() {
		if runner.watch != nil {
			runner.watch.Close()
		}
	})

	//event listener
	go func() {
		defer func() {
			if !global.Env().IsDebug {
				if r := recover(); r != nil {
					var v string
					switch r.(type) {
					case error:
						v = r.(error).Error()
					case runtime.Error:
						v = r.(runtime.Error).Error()
					case string:
						v = r.(string)
					}
					log.Errorf("error in index_backup [%v]", v)
				}
			}
		}()

		for {
			select {
			case ev := <-runner.watch.Events:
				{
					segmentID := ParseSegmentID(filepath.Base(ev.Name))
					log.Trace("File: ", ev.Name, ",", ev.Op, ",segmentID:", segmentID)
					if ev.Op&fsnotify.Write == fsnotify.Write {
						//async, merge events, upload after file idle for 5 seconds
						runner.changesChan <- &ChangedItem{
							SegmentID: segmentID,
							FilePath:  ev.Name,
							Timestamp: time.Now(),
						}
					}
				}
			case err := <-runner.watch.Errors:
				{
					log.Error("error : ", err)
					return
				}
			}
		}
	}()

	//upload checker
	go func() {
		defer func() {
			if !global.Env().IsDebug {
				if r := recover(); r != nil {
					var v string
					switch r.(type) {
					case error:
						v = r.(error).Error()
					case runtime.Error:
						v = r.(runtime.Error).Error()
					case string:
						v = r.(string)
					}
					log.Errorf("error in index_backup [%v]", v)
				}
			}
		}()

		intervalDuration := time.Second * time.Duration(cfg.UploadIntervalInSecond)
		timer := util.AcquireTimer(intervalDuration)
		defer util.ReleaseTimer(timer)

		for {
			select {
			case item := <-runner.changesChan:
				runner.changes[item.FilePath] = item
				break
			case <-timer.C:
				timer.Reset(5 * time.Second)
				temp := map[string]*ChangedItem{}
				for k, v := range runner.changes {
					log.Trace("processing: ", k)
					if time.Since(v.Timestamp) > 5*time.Second {
						log.Debug("upload file after idle>5 seconds: ", k)
						ok, err := runner.uploadFile(v)
						if err != nil {
							panic(err)
						}
						if ok {
							runner.updateLastFileUploadedTimestamp(v.IndexUUID, v.FileName, v.Timestamp.Unix())
						}
					} else {
						temp[k] = v
					}
				}
				runner.changes = temp
				break
			}
		}

	}()

	return &runner, nil
}

func (processor IndexBackupProcessor) Stop() error {
	signalChannel <- true
	return nil
}

func (processor *IndexBackupProcessor) Name() string {
	return "index_backup"
}

func (processor *IndexBackupProcessor) Process(ctx *pipeline.Context) error {
	defer func() {
		if !global.Env().IsDebug {
			if r := recover(); r != nil {
				var v string
				switch r.(type) {
				case error:
					v = r.(error).Error()
				case runtime.Error:
					v = r.(runtime.Error).Error()
				case string:
					v = r.(string)
				}
				log.Errorf("error in processor [%v], [%v]", processor.Name(), v)
				ctx.RecordError(fmt.Errorf("index backup panic: %v", r))
			}
		}
	}()

	//log.Error("processing index backup")

	//#on console
	//get cluster and index uuid
	//get which nodes from routing table
	//check if agent is installed

	meta := elastic.GetMetadata(processor.config.Elasticsearch)
	if meta == nil {
		panic(errors.Errorf("metadata for: %v was not found", processor.config.Elasticsearch))
	}

	indices, err := elastic.GetClient(processor.config.Elasticsearch).GetIndices(processor.config.Index)
	if err != nil {
		panic(err)
	}
	pathToWatch := map[string]string{}

	for indexName, indexInfo := range *indices {

		shardsTables, err := meta.GetIndexPrimaryShardsRoutingTable(indexName)
		if err != nil {
			panic(err)
		}

		for _, v := range shardsTables {
			//log.Error("node:",v.Node,",index:",v.Index,",shard:",v.Shard,",state:",v.State,",primary:",v.Primary)
			nodeInfo := meta.GetNodeInfo(v.Node)
			//log.Error(v.Node,",",nodeInfo)
			if nodeInfo != nil {
				//log.Error("node:",nodeInfo.Name,",ip:",nodeInfo.Ip,",",nodeInfo.Settings)
				path1, ok := nodeInfo.Settings["path"].(map[string]interface{})
				//log.Error(path,",",ok)
				log.Debugf("node info: %v", nodeInfo.Settings["path"])
				if ok {
					home, ok := path1["home"]
					if ok {
						//log.Error("home path:",home)
						ips := util.GetLocalIPs()
						for _, ip := range ips {
							//log.Error("checking ip:",ip," vs ",nodeInfo.Ip)
							if nodeInfo.Ip == ip || nodeInfo.Ip == "127.0.0.1" {
								var prefix = "/nodes/0/"
								if meta.GetMajorVersion() >= 8 {
									// See https://github.com/elastic/elasticsearch/pull/42489
									prefix = ""
								}
								path := path2.Join(util.ToString(home),
									"data", prefix, "indices",
									indexInfo.ID,
									util.ToString(v.Shard),
									"index")

								processor.initialCloneIndex(indexInfo.ID, path)
								pathToWatch[indexInfo.ID] = path
								break
							}
						}
					}
				}
			}
		}
	}

	//#on agent
	//each node should find shard's location
	//upload file to s3 and enable new files watch

	//upload checker
	timer := util.AcquireTimer(60 * time.Second)
	for {
		select {
		case <-ctx.Context.Done():
			return nil
		case <-timer.C:
			timer.Reset(60 * time.Second)
			for indexUUID, path := range pathToWatch {
				processor.initialCloneIndex(indexUUID, path)
			}
			break
		}
	}
}

func (processor *IndexBackupProcessor) updateLastFileUploadedTimestamp(uuid, file string, timestamp int64) {
	if uuid != "" {
		err := kv.AddValue("index_backup_last_access_timestamp", []byte(uuid+"__"+file), util.Int64ToBytes(timestamp))
		if err != nil {
			panic(err)
		}
	}
}

func (processor *IndexBackupProcessor) getLastFileUploadedTimestamp(uuid, file string) int64 {
	var lastAccessTime int64
	if uuid != "" {
		lastAccessBytes, err := kv.GetValue("index_backup_last_access_timestamp", []byte(uuid+"__"+file))
		if err == nil && len(lastAccessBytes) > 0 {
			lastAccessTime = util.BytesToInt64(lastAccessBytes)
		}
	}
	return lastAccessTime
}

func (processor *IndexBackupProcessor) initialCloneIndex(indexUUID, filePath string) {

	log.Debug("scanning index folder:", filePath)
	if !util.FileExists(filePath) {
		log.Error("index folder not exists:", filePath)
		return
	}

	filepath.Walk(filePath, func(file string, info os.FileInfo, err error) error {

		if info == nil {
			return nil
		}

		if !info.IsDir() {
			fileName := info.Name()
			fileAccessTimestamp := processor.getLastFileUploadedTimestamp(indexUUID, fileName)
			if info.ModTime().Unix() <= fileAccessTimestamp {
				log.Tracef("old file: %v, already uploaded, skip processing", info.Name())
				return nil
			}
			if processor.skipFiles.Contains(fileName) ||
				util.PrefixStr(fileName, ".") ||
				util.SuffixStr(fileName, ".tmp") {
				return nil
			}

			processor.changesChan <- &ChangedItem{
				SegmentID: ParseSegmentID(fileName),
				FilePath:  file,
				FileName:  fileName,
				Timestamp: info.ModTime(),
			}
		}
		return nil
	})

	processor.AddFileToWatch(filePath)
}

func (processor *IndexBackupProcessor) AddFileToWatch(path string) {
	if processor.watch != nil {
		err := processor.watch.Add(path)
		if err != nil {
			log.Error(err)
		}
	}
}

func (processor *IndexBackupProcessor) uploadFile(evt *ChangedItem) (bool, error) {
	path := evt.FilePath
	if !util.FileExists(path) {
		return false, nil
	}

	objName := util.TrimLeftStr(path, "/Users/medcl/Downloads/elasticsearch-7.9.0/data/nodes/0")
	log.Trace("processing file upload:", path, "=>", objName)

	if processor.config.UploadToFSRepo {
		dstPath := global.Env().GetDataDir() + "/backup/" + objName
		if processor.config.LocalRepoPath != "" {
			dstPath = processor.config.LocalRepoPath + objName
		}
		dir := filepath.Dir(dstPath)
		if !util.FileExists(dir) {
			err := os.MkdirAll(dir, 0777)
			if err != nil {
				panic(err)
			}
		}
		_, err := util.CopyFile(path, dstPath)
		return true, err
	}

	if processor.config.UploadToS3 {
		return s3.SyncUpload(path, processor.config.S3.Server,
			processor.config.S3.Location,
			processor.config.S3.Bucket,
			objName)
	}
	return false, nil
}
